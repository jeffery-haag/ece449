\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{amsmath,amsfonts,amssymb,amsthm,commath,dsfont}
%\usepackage{enumitem}
\usepackage[shortlabels]{enumitem}
\usepackage{framed}
\usepackage{xspace}
\usepackage{microtype}
\usepackage{float}
\usepackage[round]{natbib}
\usepackage{cleveref}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[breakable]{tcolorbox}
\tcbset{breakable}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{nicefrac}
\def\b1{\boldsymbol{1}}

\newcommand{\colbar}{\rule[-3mm]{.3mm}{1.5em}}
\newcommand{\rowbar}{\rule[.5ex]{1.5em}{.3mm}}
\DeclareMathOperator{\rank}{rank}
\def\balpha{\boldsymbol{\alpha}}
% following loops stolen from djhsu
\def\ddefloop#1{\ifx\ddefloop#1\else\ddef{#1}\expandafter\ddefloop\fi}
% \bbA, \bbB, ...
\def\ddef#1{\expandafter\def\csname bb#1\endcsname{\ensuremath{\mathbb{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop

% \cA, \cB, ...
\def\ddef#1{\expandafter\def\csname c#1\endcsname{\ensuremath{\mathcal{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop

% \vA, \vB, ..., \va, \vb, ...
\def\ddef#1{\expandafter\def\csname v#1\endcsname{\ensuremath{\boldsymbol{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\ddefloop

% \valpha, \vbeta, ...,  \vGamma, \vDelta, ...,
\def\ddef#1{\expandafter\def\csname v#1\endcsname{\ensuremath{\boldsymbol{\csname #1\endcsname}}}}
\ddefloop {alpha}{beta}{gamma}{delta}{epsilon}{varepsilon}{zeta}{eta}{theta}{vartheta}{iota}{kappa}{lambda}{mu}{nu}{xi}{pi}{varpi}{rho}{varrho}{sigma}{varsigma}{tau}{upsilon}{phi}{varphi}{chi}{psi}{omega}{Gamma}{Delta}{Theta}{Lambda}{Xi}{Pi}{Sigma}{varSigma}{Upsilon}{Phi}{Psi}{Omega}{ell}\ddefloop

\newcommand\T{{\scriptscriptstyle\mathsf{T}}}
\def\diag{\textup{diag}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\def\SPAN{\textup{span}}
\def\tu{\textup{u}}
\def\R{\mathbb{R}}
\def\E{\mathbb{E}}
\def\Z{\mathbb{Z}}
\def\be{\mathbf{e}}
\def\nf{\nabla f}
\def\veps{\varepsilon}
\def\cl{\textup{cl}}
\def\inte{\textup{int}}
\def\dom{\textup{dom}}
\def\Rad{\textup{Rad}}
\def\lsq{\ell_{\textup{sq}}}
\def\hcR{\widehat{\cR}}
\def\hcRl{\hcR_\ell}
\def\cRl{\cR_\ell}
\def\hcE{\widehat{\cE}}
\def\cEl{\cE_\ell}
\def\hcEl{\hcE_\ell}
\def\eps{\epsilon}
\def\1{\mathds{1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\def\srelu{\sigma_{\textup{r}}}
\def\vsrelu{\vec{\sigma_{\textup{r}}}}
\def\vol{\textup{vol}}
\def\sr{\sigma_r}
\usepackage{xcolor}
\def\hw{\textbf{[\texttt{hw3}]}\xspace}
\def\hwcode{\textbf{[\texttt{hw3code}]}\xspace}

\newcommand{\PA}[1]{\textcolor{red}{PA: #1}}
\newcommand{\ip}[2]{\left\langle #1, #2 \right \rangle}
\newcommand{\mjt}[1]{{\color{blue}\emph\textbf{[M:}~#1~\textbf{]}}}

\newtheorem{fact}{Fact}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{condition}{Condition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}







\newenvironment{Q}
{%
\clearpage
\item
}
{%
\phantom{s} %lol doesn't work
\bigskip
\textbf{Solution.}
}











\title{CS 446 / ECE 449 --- Homework 3}
\author{\emph{your NetID here}}
\date{Version 1.1}





\begin{document}
\maketitle

\noindent\textbf{Instructions.}
\begin{itemize}
  \item
    Homework is due \textbf{Thursday, March 3, at noon CST}; no late homework accepted.

  \item
    Everyone must submit individually on Gradescope under \texttt{hw3} and \texttt{hw3code}.
    Problem parts are marked with \hw and \hwcode to indicate where they are handed in.

  \item
    The ``written'' submission at \texttt{hw3} \textbf{must be typed}, and submitted in
    any format Gradescope accepts (to be safe, submit a PDF).  You may use \LaTeX, Markdown,
    Google Docs, MS Word, whatever you like; but it must be typed!

  \item
    When submitting at \texttt{hw3}, Gradescope will ask you to select pages
    for each problem; please do this precisely!

  \item
    Please make sure your NetID is clear and large on the first page of the homework.

  \item
    Your solution \textbf{must} be written in your own words.
    Please see the course webpage for full academic integrity information.
    Briefly, you may have high-level discussions with at most 3 classmates,
    whose NetIDs you should place on the first page of your solutions,
    and you should cite any external reference you use; despite all this,
    your solution must be written in your own words.

  \item
    We reserve the right to reduce the auto-graded score for
    \texttt{hw3code} if we detect funny business (e.g., your solution
    lacks any algorithm and hard-codes answers you obtained from
    someone else, or simply via trial-and-error with the autograder).

  \item
    Coding problems come with suggested ``library routines''; we include these to reduce
    your time fishing around APIs, but you are free to use other APIs.

    
  \item
    When submitting to \texttt{hw3code}, upload \texttt{hw3.py}. Donâ€™t upload a zip file or additional files.
\end{itemize}

\noindent\textbf{Version history.}
\begin{enumerate}
    \item[1.0.] Initial version.
    \item[1.1.] Clarify to use SGD in Problem 1(c) and Problem 1(d).
\end{enumerate}

\begin{enumerate}[font={\Large\bfseries},left=0pt]
  


\begin{Q}
    \textbf{\Large ResNet.}
    
    In this problem, you will implement a simplified ResNet. You do not need to change arguments which are not mentioned here (but you of course could try and see what happens).
    \begin{enumerate}
        \item \hwcode
          Implement a class \texttt{Block}, which is a building block of ResNet. It is described in Figure 2 of \citet{resnet}, but also as follows.

        The input to \texttt{Block} is of shape $(N,C,H,W)$, where $N$ denotes the batch size, $C$ denotes the number of channels, and $H$ and $W$ are the height and width of each channel. For each data example $\vx$ with shape $(C,H,W)$, the output of \texttt{block} is
        \begin{align*}%\label{eq:block}
            \texttt{Block}(\vx)=\sigma_r\del{\vx+f(\vx)},
        \end{align*}
        where $\sigma_r$ denotes the ReLU activation, and $f(\vx)$ also has shape $(C,H,W)$ and thus can be added to $\vx$. In detail, $f$ contains the following layers.
        \begin{enumerate}
            \item A \texttt{Conv2d} with $C$ input channels, $C$ output channels, kernel size 3, stride 1, padding 1, and no bias term.
            \item A \texttt{BatchNorm2d} with $C$ features.
            \item A ReLU layer.
            \item Another \texttt{Conv2d} with the same arguments as i above.
            \item Another \texttt{BatchNorm2d} with $C$ features.
        \end{enumerate}
        Because $3\times3$ kernels and padding 1 are used, the convolutional layers do not change the shape of each channel. Moreover, the number of channels are also kept unchanged. Therefore $f(\vx)$ does have the same shape as $\vx$.

        Additional instructions are given in doscstrings in \texttt{hw3.py}.
        
        \textbf{Library routines:} \texttt{torch.nn.Conv2d and torch.nn.BatchNorm2d.}
        
        \textbf{Remark:} Use \texttt{bias=False} for the \texttt{Conv2d} layers.

        \item \hwcode
          Implement a (shallow) \texttt{ResNet} consists of the following parts:
        \begin{enumerate}
            \item A \texttt{Conv2d} with 1 input channel, $C$ output channels, kernel size 3, stride 2, padding 1, and no bias term.
            \item A \texttt{BatchNorm2d} with $C$ features.
            \item A ReLU layer.
            \item A \texttt{MaxPool2d} with kernel size 2.
            \item A \texttt{Block} with $C$ channels.
            \item An \texttt{AdaptiveAvgPool2d} which for each channel takes the average of all elements.
            \item A \texttt{Linear} with $C$ inputs and 10 outputs.
        \end{enumerate}
        Additional instructions are given in doscstrings in \texttt{hw3.py}.
        
        \textbf{Library routines:} \texttt{torch.nn.Conv2d, torch.nn.BatchNorm2d, torch.nn.MaxPool2D,}
        
        \texttt{torch.nn.AdaptiveAvgPool2d and torch.nn.Linear.}
        
        \textbf{Remark:} Use \texttt{bias=False} for the \texttt{Conv2d} layer.
        
        
        \item \hw  Train your \texttt{ResNet} implemented in (b) with different choices $C\in\{1,2,4\}$ on digits data and draw the training error vs the test error curves. To make your life easier, we provide you with the starter code to load the digits data and draw the figures with different choices for $C$. Therefore, you only need to write the code to train your \texttt{ResNet} in  function \texttt{plot\_resnet\_loss\_1()}. Train your algorithms for 4000 epochs using SGD with mini batch size 128 and step size 0.1. See the docstrings in \texttt{hw3.py} for more details.  Include the resulting plot in your written handin. 
        
          For full credit, in addition to including the six train and test curves,
          include at least one complete sentence describing how the train and test error (and in particular their gap) change with $C$, which itself corresponds to a notion of model complexity as discussed in lecture.
        
         \textbf{Library routines:} \texttt{torch.nn.CrossEntropyLoss, torch.autograd.backward, torch.no\_grad, torch.optim.Optimizer.zero\_grad, torch.autograd.grad, torch.nn.Module.parameters.}
         
     \item \hw
         Train your \texttt{ResNet} implemented in (b) with $C=64$ on digits data and draw the training error vs the test error curve. To make your life easier, we provide you with the starter code to load the digits data and draw the figures with $C=64$. Therefore, you only need to write the code to train your \texttt{ResNet} in  function \texttt{plot\_resnet\_loss\_2()}. Train your algorithms for 4000 epochs using SGD with mini batch size 128 and step size 0.1. See the docstrings in \texttt{hw3.py} for more details. Notice that you can use the same implementation of training part in part (c). Include the resulting plot in your written handin. 
                
         For full credit, additionally include at least one complete sentence comparing the train and test error with those in part (c).
         
         \textbf{Library routines:} \texttt{torch.nn.CrossEntropyLoss, torch.autograd.backward, torch.no\_grad, torch.optim.Optimizer.zero\_grad, torch.autograd.grad, torch.nn.Module.parameters.}
    \end{enumerate}
\end{Q}

   
          \begin{Q}
             \textbf{\Large Decision Trees and Nearest Neighbor.}
             
 
              Consider the training and testing data sets as given in Figures~\ref{fig: training 1} and~\ref{fig: testing 1} for the sub-parts (a)-(c). For the sub-parts (d)-(f), refer to the training and testing data sets as given in Figures~\ref{fig: training 2} and~\ref{fig: testing 2}. For problems related to decision trees, either draw the decision trees unambiguously on the figure (e.g., either with drawing software, or by taking a picture of a hand drawing) and include the modified diagrams in your submission, or use unambiguous pseudocode to specify the decision trees.
              \begin{figure}[h]
                \centering
                \begin{subfigure}{0.49\columnwidth}
                \centering
                \includegraphics[width=0.99\columnwidth]{figures/training.pdf}
                \caption{Fig 1a: Training data set 1.}
                \label{fig: training 1}
                \end{subfigure}
                \begin{subfigure}{0.49\columnwidth}
                \centering
                \includegraphics[width=0.99\columnwidth]{figures/testing.pdf}
                \caption{Fig 1b: Testing data set 1.}
                \label{fig: testing 1}
                \end{subfigure}
                \end{figure}
            \begin{enumerate}
              \item \hw Define in pseudocode or draw (as above) a decision tree of depth one with integral
                and axis-aligned decision boundaries
                which achieves error at most $\frac 1 6$ on training data set 1 (\Cref{fig: training 1}).

                \textbf{Note:} ``integral and axis-aligned'' means the decision tree 
                consists of splitting rules of the form $[x_1 \geq 5]$, $[x_2<3]$, and so on.

              \item \hw Define in pseudocode or draw (as above) a decision tree (of any depth) with integral
                and axis-aligned decision boundaries
                which achieves zero error on training data set 1 (\Cref{fig: training 1}).

              \item \hw Define in pseudocode or draw (as above) a decision tree (of any depth) with integral
                and axis-aligned decision boundaries
                which achieves zero error on training data set 1 (\Cref{fig: training 1})
                but has error at least $\frac 1 4$ on 
                testing data set 1 (\Cref{fig: testing 1}).

                \clearpage
            
             \item
              \begin{figure}[h]
                \centering
                \begin{subfigure}{0.49\columnwidth}
                \centering
                \includegraphics[width=0.85\columnwidth]{figures/training_new.png}
                \caption{Fig 2a: Training data set 2.}
                \label{fig: training 2}
                \end{subfigure}
                \begin{subfigure}{0.49\columnwidth}
                \centering
                \includegraphics[width=0.85\columnwidth]{figures/test_new.png}
                \caption{Fig 2b: Testing data set 2.}
                \label{fig: testing 2}
                \end{subfigure}
              \end{figure}

    \hw          Define in pseudocode or draw (as above) a decision tree with integral and axis-aligned decision boundaries with at most two splits, which achieves zero error on training data set 2 (\Cref{fig: training 2}) and calculate its error on testing data set 2 (\Cref{fig: testing 2}).
              
              \item \hw Construct and draw a 1-nearest-neighbor classifier using training data set 2 (\Cref{fig: training 2}).  Then copy over that classifier to the corresponding testing data set 2 (\Cref{fig: testing 2}).  As discussed in class, the training error will be zero; what is the test error on testing data set 2 (\Cref{fig: testing 2})?  For full points, include both figures and at least one complete sentence stating the test error.
              
         	\item \hw Comparing the result of the decision tree from part (d) and the result of the 1-nn classifier from part (e). Which one has a smaller training error? Which one has a smaller test error? Which algorithm is more suitable here? (In case that both algorithms have the same error, state that they have the same error.)

            \end{enumerate}
          \end{Q}


\begin{Q}
   \textbf{\Large Robustness of the Majority Vote Classifier.}\\
   \def\maj{\textsc{Maj}}

    
    The purpose of this problem is to further investigate the behavior of the majority vote classifier (\textit{see slides 5-7 of lecture 12}) using Hoeffding's inequality (\textit{see slide 7 of lecture 12, and for more background, slide 14 of lecture 13}).  Simplified versions of Hoeffding's inequality are as follows.
     \begin{theorem}\label{thm: hoeffding}
       Given independent random variables $(Z_1,\ldots,Z_k)$ with $Z_i \in [0,1]$,
         \begin{equation}\label{eq: hoeffding 1}
           \Pr\sbr{\sum_{i=1}^k Z_i \geq  \sum_{i=1}^k\mathbb{E}[Z_i] + k\eps } \leq \exp\del{-2k\eps^2},
         \end{equation}
         and
         \begin{equation}\label{eq: hoeffding 2}
           \Pr\sbr{\sum_{i=1}^k Z_i \leq  \sum_{i=1}^k\mathbb{E}[Z_i] - k\eps } \leq \exp\del{-2k\eps^2}.
         \end{equation}
     \end{theorem}

     In this problem we have an odd number $n$ of classifiers $\{f_1,\ldots,f_n\}$
     and only consider their behavior
     on a fixed data example $(\vx,y)$; by classifier we mean $f_i(\vx) \in \{\pm 1\}$.
     Define the majority vote classifer $\maj$ as
     \[
       \maj(\vx;f_1,\ldots,f_n)
       := 2\cdot \1\sbr{\sum_{i=1}^n f_i(\vx) \geq 0 } - 1
       = \begin{cases}
           +1 &\sum_{i=1}^n f_i(\vx) > 0, \\
           -1 &\sum_{i=1}^n f_i(\vx) < 0,
         \end{cases}
     \]
     where we will not need to worry about ties since $n$ is odd.

     To demonstrate the utility of \Cref{thm: hoeffding} in analyzing $\maj$, suppose
     that $\Pr[ f_i(\vx) = y ] = p > 1/2$ independently for each $i$.
     Then, by defining a random variable $Z_i := \1[ f_i(\vx) \neq y]$
     and noting $\bbE [Z_i] = 1 - p$,
     \begin{align*}
       \Pr[\maj(\vx;f_1,\ldots,f_n) \neq y]
       &=
       \Pr\sbr{ \sum_{i=1}^n \1[ f_i(\vx) \neq y] \geq \frac n 2 }
       \\
       &=
       \Pr\sbr{ \sum_{i=1}^n Z_i \geq n(1-p) - \frac n 2 + np }
       \\
       &=
       \Pr\sbr{ \sum_{i=1}^n Z_i \geq n \bbE [Z_1] + n(p-1/2) }
       \\
       &\leq
       \exp\del{ -2n(p-1/2)^2 }.
     \end{align*}
     The purpose of this problem is to study the behavior of $\maj(\vx)$ when not all of the classifiers $\{f_1,\ldots,f_n\}$ are independent.
     \begin{enumerate}
       \item \hw
         Assume $n$ is divisible by $7$ and $5n/7$ is odd,
         and that of the $n$ classifiers $\{f_1,\ldots,f_n\}$,
         now only the first $5n/7$ of them (i.e., $\{f_1,\ldots,f_{5n/7}\}$) have independent errors on $\vx$.
         Specifically, $\Pr[f_i(\vx) = y] = p := 4/5$ for classifiers $\{f_1,\ldots,f_{5n/7}\}$.
         By contrast, we make no assumption on the other $2n/7$ classifiers (i.e., $\{f_{5n/7+1},\ldots,f_{n}\}$) and their errors. Now use Hoeffding's inequality to show that            
         \[
         	\Pr\sbr{ \sum_{i=1}^{5n/7} \1[ f_i(\vx) \neq y] \geq \frac{3n}{14} }\le
          \exp\del{ -\frac{n}{70}  }.
		\]
    
    	\item \hw Continuing from (a), further show that the majority vote classifier
    	over all $n$ classifiers is still good,  specifically showing  \[
    	\Pr\sbr{ \maj(\vx;f_1,\ldots,f_n) \neq y } \leq \exp\del{ -\frac{n}{70}  }.
    	\]
    	
  
  \textbf{For full points:} You need to derive the inequality $\Pr\sbr{ \maj(\vx;f_1,\dots,f_n) \neq y } \leq \exp(-n / 70)$ rigorously for ANY possible behavior of the $\frac{2n}{7}$ arbitrary classifiers.


  		\item \hw Is the probability of correctly classifying $\vx$ reasonably good in part (b) for large $n$? Do you have any interesting observations? Any answer which contains at least one complete sentence will receive full credit.

       \item \hw
         Now suppose that $n$ is divisible by $5$ and $3n/5$ is odd,
         but now only first $3n/5$ of the classifiers (i.e., $\{f_1,\ldots,f_{3n/5}\}$) have independent errors,
         and are correct with probability $\Pr[f_i(\vx) = y] = p:=2/3$. Use Hoeffding's inequality to show that 
                  \[
         \Pr\sbr{ \sum_{i=1}^{3n/5} \1[ f_i(\vx) \neq y] \leq \frac{n}{10} }\le
         \exp\del{ -\frac{n}{30}  }.
         \]
         
        \item \hw Continuing from (d), describe malicious behavior for the remaining $2n/5$ classifiers so that
        \[
        \Pr\sbr{ \maj(\vx;f_1,\ldots,f_n) = y } \leq \exp\del{ -\frac{n}{30}  }.
        \]
  
  \textbf{For full points:} Describe the malicious behavior of the arbitrary classifiers AND derive the inequality $\Pr\sbr{ \maj(\vx;f_1,\ldots,f_n) = y } \leq \exp(-n / 30)$.
  
 		\item \hw Comparing the results from part (b) and part (e), do you have any observation? Any answer which contains at least one complete sentence will receive full credit.
 		
  		 
 
         
     \end{enumerate}

\end{Q}
  


\end{enumerate}
\clearpage
\bibliography{shortbib}
\bibliographystyle{plainnat}

\end{document}
